\documentclass{article}
\usepackage[layout = a4paper]{geometry}

\usepackage{setspace}
\setstretch{1.25}
\usepackage{parskip}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage[sc]{mathpazo}
\newcommand{\Prob}{\,\mathbb{P}}
\newcommand{\df}{\,\textrm{d}}
\newcommand{\glm}{\textsc{glm}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Abbildungen und Tabellen
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf, width=.8\textwidth]{caption}

% Referenzen
\usepackage[sort]{natbib}

% Boxes
\usepackage{framed}

% Hyperlinks
\usepackage{hyperref}
\usepackage{varioref}

\title{The general linear model -- Additional exercises}
\author{Jan Vanhove\\{\small \url{https://janhove.github.io}}}

\date{Ghent, 15--17 July 2024}

% KNITR options -----------------------------------
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

# set global chunk options
opts_chunk$set(fig.path = 'figs/',
               fig.align = 'center',
               fig.show = 'hold',
               fig.pos = "tp",
               tidy = FALSE,
               prompt = FALSE,
               comment = '',
               highlight = TRUE,
               dev = 'cairo_pdf',
               cache = FALSE,
               fig.width = 5,
               fig.height = 5,
               message = FALSE,
               warning = FALSE,
               out.width = '.4\\textwidth')
opts_knit$set(global.par = TRUE)

options(formatR.arrow = TRUE,
        width = 60, 
        show.signif.stars = FALSE, 
        tibble.print_max = 7,
        tibble.print_min = 7,
        digits = 5)

set.seed(2023-07-12)
@

<<echo = FALSE>>=
par(las = 1,
    bty = "l",
    mar = c(3,3,2,1),
    mgp = c(2,.7, 0),
    tck = -.01)
@

<<echo = FALSE>>=
op <- par()
@

\begin{document}

\maketitle

\section*{Exercise 1: An experiment}
From the abstract of \citet{Vanhove2016}:
\begin{quote}
``This article investigates whether learners are able to quickly discover simple, systematic graphemic correspondence rules between their L1 and an unknown but closely related language in a setting of receptive multilingualism.

(\dots)

Eighty L1 German speakers participated in a translation task with written Dutch words, most of which had a German cognate. In the first part of the translation task, participants were shown 48 Dutch words, among which either 10 cognates containing the digraph ‹oe› (always corresponding to a German word with ‹u›) or 10 cognates with the digraph ‹ij› (corresponding to German ‹ei›). During this part, participants were given feedback in the form of the correct translation. In the second (feedback-free) part of the task, participants were shown another 150 Dutch words, among which 21 cognates with ‹oe› and 21 cognates with ‹ij›.''
\end{quote}
What I wanted to know was whether the exposure and feedback
to ten words containing a specific interlingual orthographic
correspondence (i.e., ‹oe›--‹u› or ‹ei›--‹ij›) was
sufficient for the
participants to pick up on this correspondence and use their
knowledge in translating new words containing the correspondence.

First, we read in the data and compute, for each participant,
the proportion of words in the second (feedback-free) part of the task
which they translated correctly in each category (cognates containing
‹oe›, cognates containing ‹ij›, other cognates, non-cognates).

<<>>=
library(tidyverse)
library(here)
d <- read_csv(here("data", "correspondencerules.csv"))
d_perParticipant <- d |> 
  filter(Block != "training") |> 
  group_by(Subject, LearningCondition, Category, WSTRight) |> 
  summarise(ProportionCorrect = mean(Correct == "yes"),
            .groups = "drop")
@

I've also retained a measure of the participants' L1 (German)
vocabulary knowledge, namely the \texttt{WSTRight} variable.
Use \texttt{View(d\_perParticipant)} to inspect the structure
of the cleaned-up and restructured dataset.

Analyse this dataset to answer the research question.

Hints (also for other exercises):
\begin{itemize}
\item There are several acceptable ways of analysing these data.
\item All of them involve plotting the data :)
\item Note that \texttt{LearningCondition} varies \emph{between}
      participants whereas \texttt{Category} varies \emph{within}
      participants.
\item First try to work out on a piece of paper what it is you want to do.
      You can always ask someone to help you implement your idea in R.
\item Ask for help and feedback! :)
\end{itemize}

\section*{Exercise 2: An exercise in plotting}
\citet{Pestana_HELASCOT_reading}
measured the Portuguese reading skills of
Portuguese children in Portugal,
French-speaking Switzerland,
and German-speaking Switzerland (\texttt{LanguageGroup}) at three points
in time (\texttt{Time}).

Let's read in their data. We'll just
retain the reading data in Portuguese.
<<>>=
skills <- read_csv(here("data", "helascot_skills.csv"))
background <- read_csv(here("data", "helascot_background.csv"))
d <- skills |> 
  left_join(background, by = "Subject") |> 
  filter(LanguageTested == "Portuguese") |> 
  filter(!is.na(Reading))
@

Since the \texttt{Time} variable is just
a number, we'll recode it as a factor:
<<>>=
d$Time <- factor(d$Time)
@

Your task:
Draw a plot showing how the reading scores differ between the different data collections (\texttt{Time}) and the language groups.

\section*{Exercise 3}
From the abstract of \citet{Hicks2021}:
\begin{quotation}
``This study explores whether middle-school students can exploit explicitly addressed crosslinguistic lexical similarities between German and English to learn vocabulary more efficiently. Across six weeks, 260 Swiss German learners of English as a foreign language (17 classes) completed three vocabulary learning tests (T1, T2 and T3). Additionally, 7 of these 17 classes attended a 90-minute intervention between the first and second test: During a 45-minute introductory lesson students discovered four systematic orthographic correspondence rules (e.g. <p> to <f> as in ship and Schiff), followed by three 15-minute sessions to consolidate their knowledge.''
\end{quotation}
She was interested in whether the children in the experimental condition were able to 
learn vocabulary more efficiently than the children in the control condition.

The dataset contains lots of variables that aren't of interest for this
exercise, so let's discard those:
<<>>=
d <- read_csv(here("data", "hicks2021.csv")) |> 
  select(ID, Class, Group, T1cog, T3cog)
@
\texttt{ID} contains the participants' study-internal identification;
\texttt{class} identifies their school class;
\texttt{Group} the experimental condition to which they were assigned;
\texttt{T1cog} is the participants' pretest performance;
\texttt{T3cog} is the participants' posttest performance.

Task:
\begin{enumerate}
  \item Assume the participants were randomly, and individually, assigned
  to the experimental conditions. Analyse the data.
  
  \item In actual fact, all participants in the same class were assigned
  to the same condition. In other words, the participants were not assigned
  to the conditions individually. What kind of problems might this cause for 
  the analysis?
  
  \item Moreover, the participants weren't randomly assigned to the conditions.
  Those classes whose English teachers agreed to be part of the intervention
  group were assigned to the experimental group; those whose English teachers
  did not agree to do so were assigned to the control group. What kind of problems
  might this have caused?
\end{enumerate}

\section*{Exercise 4: Meaning of model parameters}

<<echo = FALSE, message = FALSE>>=
cognates <- read_csv(here("data", "vanhove2014_cognates.csv"))
background <- read_csv(here("data", "vanhove2014_background.csv"))
d <- cognates %>%   
  left_join(background, by = "Subject") %>%
   filter(English.Overall != -9999)
 
d$n.Sex <- ifelse(d$Sex == "male", 0.5, -0.5)
options(digits = 2)
@

We're given a data set \texttt{d} with the outcome \texttt{CorrectSpoken}
and four predictors:
\begin{itemize}
  \item \texttt{n.Sex}: $0.5$ for men, $-0.5$ for women (sum coding).
  \item \texttt{NrLang}: Number of languages spoken for each participant (varies from 1 to 5).
  \item \texttt{DS.Span}: Score on the backward digit span, a working memory test (varies from 2 to 8, more is better).
  \item \texttt{Raven.Right}: Score on a test of fluid intelligence (varies from 0 to 35, more is better).
\end{itemize}

We fit the following model:
<<>>=
mod.lm <- lm(CorrectSpoken ~ NrLang + DS.Span + n.Sex*Raven.Right,
             data = d)
summary(mod.lm)$coefficients
@

\begin{enumerate}
\item What does the estimate of $-5.02$ for the \texttt{n.Sex} parameter refer to? That is, what does it mean literally.

\item Let's say we wanted to fit a similar model but one that has an intercept
      with a more useful interpretation. Explain how we could achieve this.
      Also explain how we can interpret the intercept in this new model.

\item Claim: We can glean from the model that, at least in this data set,
      participants with a high working memory capacity do not outperform
      participants with a low working memory capacity in terms of the \texttt{CorrectSpoken} variable. \\
      Is this claim correct? (Yes or no.) Justify your answer.

\item What gets computed here?
<<>>=
8.554 + 1.115*3 - 0.097*4 + 0.286*25
@

\item How large, according to the model, is the expected average difference 
in the \texttt{CorrectSpoken} variable between women with a
\texttt{Raven.Right} score of 20 and women with a \texttt{Raven.Right} score
of 30, keeping \texttt{NrLang} and \texttt{DS.Span} constant. 
It suffices to provide the computation without evaluating it.
\end{enumerate}



\bibliographystyle{unified}
\bibliography{bibliography}

\end{document}
